{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "import argparse\n",
    "import contextlib\n",
    "import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from itertools import chain, combinations\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import altair as alt\n",
    "import altair_viewer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peewee\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "from json_tricks import dumps, loads\n",
    "from playhouse.shortcuts import model_to_dict\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.datasets import load_iris\n",
    "from tabulate import tabulate\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from active_learning.cluster_strategies import (\n",
    "    DummyClusterStrategy,\n",
    "    MostUncertainClusterStrategy,\n",
    "    RandomClusterStrategy,\n",
    "    RoundRobinClusterStrategy,\n",
    ")\n",
    "from active_learning.dataStorage import DataStorage\n",
    "from active_learning.experiment_setup_lib import (\n",
    "    ExperimentResult,\n",
    "    get_db,\n",
    "    get_single_al_run_stats_row,\n",
    "    get_single_al_run_stats_table_header,\n",
    ")\n",
    "from active_learning.sampling_strategies import (\n",
    "    BoundaryPairSampler,\n",
    "    CommitteeSampler,\n",
    "    RandomSampler,\n",
    "    UncertaintySampler,\n",
    ")\n",
    "\n",
    "alt.renderers.enable(\"altair_viewer\")\n",
    "#  alt.renderers.enable('vegascope')\n",
    "\n",
    "config = {\n",
    "    \"datasets_path\": \"../datasets\",\n",
    "    \"db\": \"tunnel\",\n",
    "    \"param_list_id\": \"best_global_score\",\n",
    "}\n",
    "\n",
    "db = get_db(db_name_or_type=config[\"db\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14,579 dwtc\n"
     ]
    }
   ],
   "source": [
    "# select count(*), dataset_name from experimentresult group by dataset_name;\n",
    "results = ExperimentResult.select(\n",
    "    ExperimentResult.dataset_name,\n",
    "    peewee.fn.COUNT(ExperimentResult.id_field).alias(\"dataset_name_count\"),\n",
    ").group_by(ExperimentResult.dataset_name)\n",
    "\n",
    "for result in results:\n",
    "    print(\"{:>4,d} {}\".format(result.dataset_name_count, result.dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT id_field, param_list_id, dataset_path, start_set_size as sss, sampling, cluster, allow_recommendations_after_stop as SA, stopping_criteria_uncertainty as SCU, stopping_criteria_std as SCS, stopping_criteria_acc as SCA, amount_of_user_asked_queries as \"#q\", acc_test, fit_score, global_score_norm, thread_id, end_time from experimentresult where param_list_id='31858014d685a3f1ba3e4e32690ddfc3' order by end_time, fit_score desc, param_list_id;\n",
    "loaded_data = {}\n",
    "\n",
    "\n",
    "def pre_fetch_data(top_n=0):\n",
    "    best_param_list_id = table[top_n][\"param_list_id\"]\n",
    "\n",
    "    results = (\n",
    "        ExperimentResult.select()\n",
    "        .where(ExperimentResult.param_list_id == best_param_list_id)\n",
    "        .order_by(ExperimentResult.dataset_name)\n",
    "    )\n",
    "\n",
    "    loaded_data[top_n] = []\n",
    "    for result in results:\n",
    "        loaded_data[top_n].append(result)\n",
    "    print(\"Loaded Top \" + str(top_n) + \" data\")\n",
    "\n",
    "\n",
    "# pre_fetch_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  id</th><th style=\"text-align: right;\">  avg_acc_test</th><th style=\"text-align: right;\">  avg_fit_score</th><th>stddev_fit_score  </th><th style=\"text-align: right;\">  avg_amount_oracle</th><th>std_amount_oracle  </th><th style=\"text-align: right;\">  count</th><th>param_list_id                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">      0.823468</td><td style=\"text-align: right;\">       0.872299</td><td>                  </td><td style=\"text-align: right;\">                210</td><td>                   </td><td style=\"text-align: right;\">      1</td><td>301ef9fa361fef2436ce2605eafccf9e</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'test_data_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-3bba22652386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mbetter_results_top\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m211\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0mbetter_results_top\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m211\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mbetter_results_top\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m211\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-3bba22652386>\u001b[0m in \u001b[0;36mbetter_results_top\u001b[0;34m(top_n, budget, weak_clust, weak_cert)\u001b[0m\n\u001b[1;32m     64\u001b[0m         test_data_metrics = [\n\u001b[1;32m     65\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_data_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_data_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         ]\n\u001b[1;32m     68\u001b[0m         test_acc = [\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test_data_metrics'"
     ]
    }
   ],
   "source": [
    "def better_results_top(top_n, budget, weak_clust, weak_cert):\n",
    "    # select best result for budget of 1500 without WS\n",
    "    results = (\n",
    "        ExperimentResult.select(\n",
    "            ExperimentResult.param_list_id,\n",
    "            peewee.fn.AVG(ExperimentResult.acc_test).alias(\"avg_acc_test\"),\n",
    "            peewee.fn.AVG(ExperimentResult.fit_score).alias(\"avg_fit_score\"),\n",
    "            peewee.fn.STDDEV(ExperimentResult.fit_score).alias(\"stddev_fit_score\"),\n",
    "      \n",
    "            peewee.fn.AVG(ExperimentResult.amount_of_user_asked_queries).alias(\n",
    "                \"avg_amount_oracle\"\n",
    "            ),\n",
    "            peewee.fn.STDDEV(ExperimentResult.amount_of_user_asked_queries).alias(\n",
    "                \"std_amount_oracle\"\n",
    "            ),\n",
    "            peewee.fn.COUNT(ExperimentResult.param_list_id).alias(\"count\"),\n",
    "        )\n",
    "        .where(\n",
    "            (ExperimentResult.amount_of_user_asked_queries < budget)\n",
    "            & (ExperimentResult.dataset_name == \"dwtc\")\n",
    "            # & (ExperimentResult.experiment_run_date > (datetime(2020, 3, 24, 14, 0)))\n",
    "            # & (ExperimentResult.experiment_run_date > (datetime(2020, 5, 8, 9, 20)))\n",
    "            & (ExperimentResult.with_cluster_recommendation == weak_clust)\n",
    "            & (ExperimentResult.with_uncertainty_recommendation == weak_cert)\n",
    "            # & (peewee.fn.COUNT(ExperimentResult.id_field) == 3)\n",
    "            # no stopping criterias\n",
    "        )\n",
    "        .group_by(ExperimentResult.param_list_id)\n",
    "        .order_by(\n",
    "            peewee.fn.COUNT(ExperimentResult.id_field).desc(),\n",
    "            peewee.fn.AVG(ExperimentResult.acc_test).desc(),\n",
    "        )\n",
    "        .limit(1)\n",
    "        .offset(top_n)\n",
    "    )\n",
    "\n",
    "    table = []\n",
    "    id = 0\n",
    "    for result in results:\n",
    "        data = {**{\"id\": id}, **vars(result)}\n",
    "        data[\"param_list_id\"] = data[\"__data__\"][\"param_list_id\"]\n",
    "        del data[\"__data__\"]\n",
    "        del data[\"_dirty\"]\n",
    "        del data[\"__rel__\"]\n",
    "        table.append(data)\n",
    "        id += 1\n",
    "\n",
    "    display(HTML(tabulate(table, headers=\"keys\", tablefmt=\"html\")))\n",
    "\n",
    "    best_param_list_id = table[0][\"param_list_id\"]\n",
    "\n",
    "    results = (\n",
    "        ExperimentResult.select()\n",
    "        .where(ExperimentResult.param_list_id == best_param_list_id)\n",
    "        .order_by(ExperimentResult.dataset_name)\n",
    "    )\n",
    "\n",
    "    loaded_data[0] = []\n",
    "    for result in results:\n",
    "        loaded_data[0].append(result)\n",
    "\n",
    "    for result in loaded_data[0][:]:\n",
    "        metrics = loads(result.metrics_per_al_cycle)\n",
    "        test_data_metrics = [\n",
    "            metrics[\"test_data_metrics\"][0][f][0][\"weighted avg\"]\n",
    "            for f in range(0, len(metrics[\"test_data_metrics\"][0]))\n",
    "        ]\n",
    "        test_acc = [\n",
    "            metrics[\"test_data_metrics\"][0][f][0][\"accuracy\"]\n",
    "            for f in range(0, len(metrics[\"test_data_metrics\"][0]))\n",
    "        ]\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                \"iteration\": range(0, len(metrics[\"all_unlabeled_roc_auc_scores\"])),\n",
    "                \"all_unlabeled_roc_auc_scores\": metrics[\"all_unlabeled_roc_auc_scores\"],\n",
    "                \"query_length\": metrics[\"query_length\"],\n",
    "                \"recommendation\": metrics[\"recommendation\"],\n",
    "                \"query_strong_accuracy_list\": metrics[\"query_strong_accuracy_list\"],\n",
    "                \"f1\": [i[\"f1-score\"] for i in test_data_metrics],\n",
    "                \"test_acc\": test_acc,\n",
    "                \"fit_score\": result.fit_score,\n",
    "                #'asked_queries': [sum(metrics['query_length'][:i]) for i in range(0, len(metrics['query_length']))],\n",
    "            }\n",
    "        )\n",
    "        data[\"acc_diff\"] = data[\"test_acc\"] - data[\"test_acc\"].shift(1)\n",
    "    display(\n",
    "        HTML(\n",
    "            tabulate(\n",
    "                data.groupby([\"recommendation\"]).sum(), headers=\"keys\", tablefmt=\"html\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "better_results_top(0, 211, False, False)\n",
    "better_results_top(0, 211, True, False)\n",
    "better_results_top(0, 211, False, True)\n",
    "for i in range(1, 2):\n",
    "    better_results_top(i, 211, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import pickle\n",
    "\n",
    "#  SELECT param_list_id, avg(fit_score), stddev(fit_score), avg(global_score), stddev(global_score), avg(start_set_size) as sss, count(*) FROM experimentresult WHERE start_set_size = 1 GROUP BY param_list_id ORDER BY 7 DESC, 4 DESC LIMIT 30;\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# & (ExperimentResult.experiment_run_date > (datetime(2020, 3, 24, 14, 0))) # no stopping criterias\n",
    "#  & (ExperimentResult.experiment_run_date > (datetime(2020, 3, 30, 12, 23))) # optics\n",
    "\n",
    "\n",
    "results = (\n",
    "    ExperimentResult.select(ExperimentResult.param_list_id,)\n",
    "    .where(\n",
    "        # (ExperimentResult.amount_of_user_asked_queries < 211)\n",
    "        (ExperimentResult.dataset_name == \"dwtc\")\n",
    "        # & (ExperimentResult.experiment_run_date > (datetime(2020, 3, 24, 14, 0)))\n",
    "        # & (ExperimentResult.experiment_run_date > (datetime(2020, 5, 8, 9, 20)))\n",
    "        # & (ExperimentResult.with_cluster_recommendation == True)\n",
    "        # & (ExperimentResult.with_uncertainty_recommendation == True)\n",
    "        # & (peewee.fn.COUNT(ExperimentResult.id_field) == 3)\n",
    "        # no stopping criterias\n",
    "    )\n",
    "    .order_by(ExperimentResult.acc_test.desc())\n",
    "    .limit(10000)\n",
    ")\n",
    "\n",
    "table = []\n",
    "id = 0\n",
    "for result in results:\n",
    "    data = {**{\"id\": id}, **vars(result)}\n",
    "    data[\"param_list_id\"] = data[\"__data__\"][\"param_list_id\"]\n",
    "    del data[\"__data__\"]\n",
    "    del data[\"_dirty\"]\n",
    "    del data[\"__rel__\"]\n",
    "\n",
    "    # get one param_list_id\n",
    "\n",
    "    one_param_list_id_result = (\n",
    "        ExperimentResult.select(\n",
    "            ExperimentResult.acc_test_oracle,\n",
    "            ExperimentResult.acc_test,\n",
    "            ExperimentResult.fit_score,\n",
    "            ExperimentResult.fit_time,\n",
    "            ExperimentResult.amount_of_all_labels,\n",
    "            ExperimentResult.amount_of_user_asked_queries,\n",
    "            # ExperimentResult.classifier,\n",
    "            # ExperimentResult.test_fraction,\n",
    "            ExperimentResult.sampling,\n",
    "            ExperimentResult.cluster,\n",
    "            # ExperimentResult.nr_queries_per_iteration,\n",
    "            ExperimentResult.with_uncertainty_recommendation,\n",
    "            ExperimentResult.with_cluster_recommendation,\n",
    "            ExperimentResult.uncertainty_recommendation_certainty_threshold,\n",
    "            ExperimentResult.uncertainty_recommendation_ratio,\n",
    "            ExperimentResult.cluster_recommendation_minimum_cluster_unity_size,\n",
    "            ExperimentResult.cluster_recommendation_ratio_labeled_unlabeled,\n",
    "            ExperimentResult.allow_recommendations_after_stop,\n",
    "            ExperimentResult.experiment_run_date,\n",
    "        )\n",
    "        .where(ExperimentResult.param_list_id == data[\"param_list_id\"])\n",
    "        .limit(1)\n",
    "    )[0]\n",
    "    data[\"weak?\"] = operator.and_(\n",
    "        operator.and_(\n",
    "            operator.or_(\n",
    "                one_param_list_id_result.with_uncertainty_recommendation,\n",
    "                one_param_list_id_result.with_cluster_recommendation,\n",
    "            ),\n",
    "            one_param_list_id_result.amount_of_all_labels > 214,\n",
    "        ),\n",
    "        one_param_list_id_result.acc_test > one_param_list_id_result.acc_test_oracle,\n",
    "    )\n",
    "\n",
    "    data[\"acc_test_all_better?\"] = (\n",
    "        one_param_list_id_result.acc_test > one_param_list_id_result.acc_test_oracle\n",
    "    )\n",
    "    data[\"true_weak?\"] = operator.and_(\n",
    "        operator.or_(\n",
    "            one_param_list_id_result.with_uncertainty_recommendation,\n",
    "            one_param_list_id_result.with_cluster_recommendation,\n",
    "        ),\n",
    "        one_param_list_id_result.amount_of_all_labels > 214,\n",
    "    )\n",
    "    data[\"interesting?\"] = operator.and_(\n",
    "        data[\"true_weak?\"], data[\"acc_test_all_better?\"]\n",
    "    )\n",
    "    data = {**data, **vars(one_param_list_id_result)[\"__data__\"]}\n",
    "\n",
    "    table.append(data)\n",
    "    id += 1\n",
    "\n",
    "with open(\"200er_results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(table, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# display(HTML(tabulate(table, headers=\"keys\", tablefmt=\"html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
